{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutoGluonVenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import time\n",
    "\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from autogluon.core.metrics import make_scorer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def reduce_mem_usage_pandas(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Redukcja zużycia pamięci w Pandas poprzez downcast typów numerycznych i konwersję do kategorii.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if pd.api.types.is_integer_dtype(col_type):\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n",
    "                df[col] = df[col].astype(np.int8)\n",
    "            elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n",
    "                df[col] = df[col].astype(np.int16)\n",
    "            elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n",
    "                df[col] = df[col].astype(np.int32)\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.int64)\n",
    "        \n",
    "        elif pd.api.types.is_float_dtype(col_type):\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "        \n",
    "        elif pd.api.types.is_object_dtype(col_type):\n",
    "            num_unique_values = df[col].nunique()\n",
    "            num_total_values = len(df[col])\n",
    "            if num_unique_values / num_total_values < 0.5:\n",
    "                df[col] = df[col].astype('category')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def reduce_mem_usage_polars(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Redukcja pamięci w Polars przez downcast typów numerycznych.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        dtype = df[col].dtype\n",
    "        # Sprawdź typy\n",
    "        if dtype == pl.Int64:\n",
    "            col_min = df[col].min()\n",
    "            col_max = df[col].max()\n",
    "            if col_min >= np.iinfo(np.int8).min and col_max <= np.iinfo(np.int8).max:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int8))\n",
    "            elif col_min >= np.iinfo(np.int16).min and col_max <= np.iinfo(np.int16).max:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int16))\n",
    "            elif col_min >= np.iinfo(np.int32).min and col_max <= np.iinfo(np.int32).max:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int32))\n",
    "        elif dtype == pl.Float64:\n",
    "            col_min = df[col].min()\n",
    "            col_max = df[col].max()\n",
    "            # Można zdecydować się np. tylko na Float32\n",
    "            df = df.with_columns(pl.col(col).cast(pl.Float32))\n",
    "    return df\n",
    "\n",
    "class CONFIG:\n",
    "    target_col = \"responder_6\"\n",
    "    lag_cols_original = [\"date_id\", \"symbol_id\"] + [f\"responder_{idx}\" for idx in range(9)]\n",
    "    valid_ratio = 0.1\n",
    "    start_dt = 1100\n",
    "    lags_cols = [f\"responder_{i}_lag_1\" for i in range(9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data with Polars...\n"
     ]
    }
   ],
   "source": [
    "features = [f'feature_{i:02d}' for i in range(79)]\n",
    "target = CONFIG.target_col\n",
    "weight_col = 'weight'\n",
    "\n",
    "train_path1 = 'train.parquet'\n",
    "\n",
    "print(\"Reading training data with Polars...\")\n",
    "df_polars = pl.read_parquet(train_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing memory usage in Polars (df_polars)...\n"
     ]
    }
   ],
   "source": [
    "print(\"Reducing memory usage in Polars (df_polars)...\")\n",
    "df_polars = reduce_mem_usage_polars(df_polars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_polars(\n",
    "    df: pl.DataFrame,\n",
    "    features: list,\n",
    "    target: str,\n",
    "    weight_col: str,\n",
    "    fallback_value: float = 3.0\n",
    ") -> pl.DataFrame:\n",
    "    required_columns = set(features + [target, weight_col])\n",
    "    missing_columns = required_columns - set(df.columns)\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing columns in dataset: {missing_columns}\")\n",
    "\n",
    "    print(\"Missing values per column before imputation (Polars):\")\n",
    "    missing_before = df.select(\n",
    "        [pl.col(col).is_null().sum().alias(f\"{col}_missing\") for col in required_columns]\n",
    "    )\n",
    "    print(missing_before)\n",
    "    fill_expressions = []\n",
    "    for col in features:\n",
    "        fill_expressions.append(pl.col(col).fill_null(fallback_value))\n",
    "    fill_expressions.append(pl.col(weight_col).fill_null(1.9))\n",
    "    df = df.with_columns(fill_expressions)\n",
    "\n",
    "    weight_array = df[weight_col].to_numpy().reshape(-1, 1)\n",
    "    scaler = StandardScaler()\n",
    "    weight_scaled = scaler.fit_transform(weight_array).flatten()\n",
    "    \n",
    "    df = df.with_columns([\n",
    "        pl.Series(weight_col, weight_scaled).alias(weight_col).cast(pl.Float32)\n",
    "    ])\n",
    "\n",
    "    # Logowanie brakujących wartości po imputacji\n",
    "    print(\"Missing values per column after imputation (Polars):\")\n",
    "    missing_after = df.select(\n",
    "        [pl.col(col).is_null().sum().alias(f\"{col}_missing\") for col in required_columns]\n",
    "    )\n",
    "    print(missing_after)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column before imputation (Polars):\n",
      "shape: (1, 81)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ feature_4 ┆ feature_0 ┆ feature_0 ┆ feature_1 ┆ … ┆ feature_3 ┆ feature_0 ┆ feature_4 ┆ feature_ │\n",
      "│ 8_missing ┆ 3_missing ┆ 4_missing ┆ 8_missing ┆   ┆ 6_missing ┆ 9_missing ┆ 3_missing ┆ 46_missi │\n",
      "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ng       │\n",
      "│ u32       ┆ u32       ┆ u32       ┆ u32       ┆   ┆ u32       ┆ u32       ┆ u32       ┆ ---      │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ u32      │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ 0         ┆ 0         ┆ 0         ┆ 0         ┆ … ┆ 0         ┆ 0         ┆ 40        ┆ 1248     │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n",
      "Missing values per column after imputation (Polars):\n",
      "shape: (1, 81)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ feature_4 ┆ feature_0 ┆ feature_0 ┆ feature_1 ┆ … ┆ feature_3 ┆ feature_0 ┆ feature_4 ┆ feature_ │\n",
      "│ 8_missing ┆ 3_missing ┆ 4_missing ┆ 8_missing ┆   ┆ 6_missing ┆ 9_missing ┆ 3_missing ┆ 46_missi │\n",
      "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ng       │\n",
      "│ u32       ┆ u32       ┆ u32       ┆ u32       ┆   ┆ u32       ┆ u32       ┆ u32       ┆ ---      │\n",
      "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ u32      │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ 0         ┆ 0         ┆ 0         ┆ 0         ┆ … ┆ 0         ┆ 0         ┆ 0         ┆ 0        │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n",
      "Tworzenie lagów w Polars za pomocą window functions...\n"
     ]
    }
   ],
   "source": [
    "df_polars = preprocess_data_polars(\n",
    "    df_polars,\n",
    "    features=features,\n",
    "    target=target,\n",
    "    weight_col=weight_col,\n",
    "    fallback_value=3.0\n",
    ")\n",
    "\n",
    "print(\"Tworzenie lagów w Polars za pomocą window functions...\")\n",
    "\n",
    "df_polars = df_polars.sort([\"symbol_id\", \"date_id\"])\n",
    "\n",
    "lag_expressions = [\n",
    "    pl.col(f\"responder_{i}\").shift(1).over(\"symbol_id\").alias(f\"responder_{i}_lag_1\")\n",
    "    for i in range(9)\n",
    "]\n",
    "\n",
    "df_polars = df_polars.with_columns(lag_expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reducing memory usage in Polars after adding lags...\n",
      "Podział na zestawy treningowe i walidacyjne...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"Reducing memory usage in Polars after adding lags...\")\n",
    "df_polars = reduce_mem_usage_polars(df_polars)\n",
    "\n",
    "print(\"Podział na zestawy treningowe i walidacyjne...\")\n",
    "\n",
    "df = df_polars.to_pandas()\n",
    "\n",
    "df = reduce_mem_usage_pandas(df)\n",
    "\n",
    "del df_polars\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " len_train = 4239026\n",
      " len_ofl_mdl = 3815124\n",
      "---> Last offline train date = 1604\n",
      "\n",
      "Zapis danych do plików Parquet...\n",
      "Rozpoczynam trenowanie modelu AutoGluon...\n"
     ]
    }
   ],
   "source": [
    "categorical_cols = ['symbol_id']\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "len_train = len(df)\n",
    "valid_records = int(len_train * CONFIG.valid_ratio)\n",
    "len_ofl_mdl = len_train - valid_records\n",
    "\n",
    "df = df.sort_values('date_id').reset_index(drop=True)\n",
    "\n",
    "last_tr_dt = df['date_id'].iloc[len_ofl_mdl]\n",
    "print(f\"\\n len_train = {len_train}\")\n",
    "print(f\" len_ofl_mdl = {len_ofl_mdl}\")\n",
    "print(f\"---> Last offline train date = {last_tr_dt}\\n\")\n",
    "\n",
    "validation_data = df[df['date_id'] > last_tr_dt]\n",
    "\n",
    "del df\n",
    "\n",
    "gc.collect()\n",
    "train_features = features + CONFIG.lags_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.common.features.feature_metadata import FeatureMetadata\n",
    "\n",
    "def generate_feature_metadata_pandas(df: pd.DataFrame, features: list, categorical_threshold: int = 50) -> FeatureMetadata:\n",
    "    \"\"\"\n",
    "    Generuje obiekt FeatureMetadata na podstawie typów danych w Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Pandas DataFrame.\n",
    "        features (list): Lista nazw cech.\n",
    "        categorical_threshold (int): Próg liczby unikalnych wartości, poniżej którego cecha jest traktowana jako kategoryczna.\n",
    "\n",
    "    Returns:\n",
    "        FeatureMetadata: Obiekt FeatureMetadata.\n",
    "    \"\"\"\n",
    "    type_map_raw = {}\n",
    "    type_group_map_special = {}\n",
    "\n",
    "    for feature in features:\n",
    "        dtype = df[feature].dtype\n",
    "        if pd.api.types.is_integer_dtype(dtype):\n",
    "            type_map_raw[feature] = 'int'\n",
    "        elif pd.api.types.is_float_dtype(dtype):\n",
    "            type_map_raw[feature] = 'float'\n",
    "        elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
    "            type_map_raw[feature] = 'datetime'\n",
    "        elif pd.api.types.is_categorical_dtype(dtype):\n",
    "            type_map_raw[feature] = 'category'\n",
    "        elif pd.api.types.is_object_dtype(dtype):\n",
    "            unique_count = df[feature].nunique()\n",
    "            if unique_count <= categorical_threshold:\n",
    "                type_map_raw[feature] = 'category'\n",
    "            else:\n",
    "                type_map_raw[feature] = 'object'\n",
    "        else:\n",
    "            type_map_raw[feature] = 'object'\n",
    "\n",
    "    feature_metadata = FeatureMetadata(type_map_raw=type_map_raw, type_group_map_special=type_group_map_special)\n",
    "    return feature_metadata\n",
    "\n",
    "feature_metadata = generate_feature_metadata_pandas(training_data, train_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Verbosity: 3 (Detailed Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.2\n",
      "Python Version:     3.10.12\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          32\n",
      "GPU Count:          1\n",
      "Memory Avail:       23.77 GB / 31.34 GB (75.8%)\n",
      "Disk Space Avail:   68.62 GB / 481.95 GB (14.2%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality']\n",
      "============ fit kwarg info ============\n",
      "User Specified kwargs:\n",
      "{'ag_args_fit': {'num_gpus': 1, 'num_stack_levels': 2},\n",
      " 'auto_stack': True,\n",
      " 'num_bag_folds': 5}\n",
      "Full kwargs:\n",
      "{'_feature_generator_kwargs': None,\n",
      " '_save_bag_folds': None,\n",
      " 'ag_args': None,\n",
      " 'ag_args_ensemble': None,\n",
      " 'ag_args_fit': {'num_gpus': 1, 'num_stack_levels': 2},\n",
      " 'auto_stack': True,\n",
      " 'calibrate': 'auto',\n",
      " 'delay_bag_sets': False,\n",
      " 'ds_args': {'clean_up_fits': True,\n",
      "             'detection_time_frac': 0.25,\n",
      "             'enable_callbacks': False,\n",
      "             'enable_ray_logging': True,\n",
      "             'holdout_data': None,\n",
      "             'holdout_frac': 0.1111111111111111,\n",
      "             'memory_safe_fits': True,\n",
      "             'n_folds': 2,\n",
      "             'n_repeats': 1,\n",
      "             'validation_procedure': 'holdout'},\n",
      " 'excluded_model_types': None,\n",
      " 'feature_generator': 'auto',\n",
      " 'feature_prune_kwargs': None,\n",
      " 'holdout_frac': None,\n",
      " 'hyperparameter_tune_kwargs': None,\n",
      " 'included_model_types': None,\n",
      " 'keep_only_best': False,\n",
      " 'learning_curves': False,\n",
      " 'name_suffix': None,\n",
      " 'num_bag_folds': 5,\n",
      " 'num_bag_sets': None,\n",
      " 'num_stack_levels': None,\n",
      " 'pseudo_data': None,\n",
      " 'raise_on_no_models_fitted': True,\n",
      " 'refit_full': False,\n",
      " 'save_bag_folds': None,\n",
      " 'save_space': False,\n",
      " 'set_best_to_refit_full': False,\n",
      " 'test_data': None,\n",
      " 'unlabeled_data': None,\n",
      " 'use_bag_holdout': False,\n",
      " 'verbosity': 3}\n",
      "========================================\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=5, num_bag_sets=1\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/learner.pkl\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/predictor.pkl\n",
      "Values in column 'weight' used as sample weights instead of predictive features. Evaluation metrics will ignore sample weights, specify weight_evaluation=True to instead report weighted metrics.\n",
      "Beginning AutoGluon training ... Time limit = 43200s\n",
      "AutoGluon will save models to \"/mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301\"\n",
      "Train Data Rows:    4479599\n",
      "Train Data Columns: 89\n",
      "Label Column:       responder_6\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    25695.77 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1503.77 MB (5.9% of available memory)\n",
      "\tWarning: Data size prior to feature transformation consumes 5.9% of available memory. Consider increasing memory or subsampling the data to avoid instability.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tOriginal Features (exact raw dtype, raw dtype):\n",
      "\t\t\t\t('float32', 'float') : 88 | ['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', ...]\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 88 | ['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 88 | ['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', ...]\n",
      "\t\t\t2.9s = Fit runtime\n",
      "\t\t\t88 features in original data used to generate 88 features in processed data.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 88 | ['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 88 | ['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', ...]\n",
      "\t\t\t0.6s = Fit runtime\n",
      "\t\t\t88 features in original data used to generate 88 features in processed data.\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 88 | ['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 88 | ['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', ...]\n",
      "\t\t\t0.5s = Fit runtime\n",
      "\t\t\t88 features in original data used to generate 88 features in processed data.\n",
      "\t\tSkipping CategoryFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping DatetimeFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping TextSpecialFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping TextNgramFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IdentityFeatureGenerator: No input feature with required dtypes.\n",
      "\t\tSkipping IsNanFeatureGenerator: No input feature with required dtypes.\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 88 | ['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 88 | ['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', ...]\n",
      "\t\t\t21.0s = Fit runtime\n",
      "\t\t\t88 features in original data used to generate 88 features in processed data.\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 88 | ['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', ...]\n",
      "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t\t\t('float', []) : 88 | ['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', ...]\n",
      "\t\t\t0.5s = Fit runtime\n",
      "\t\t\t88 features in original data used to generate 88 features in processed data.\n",
      "\tTypes of features in original data (exact raw dtype, raw dtype):\n",
      "\t\t('float32', 'float') : 88 | ['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 88 | ['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', ...]\n",
      "\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
      "\t\t('float32', 'float') : 88 | ['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 88 | ['feature_00', 'feature_01', 'feature_02', 'feature_03', 'feature_04', ...]\n",
      "\t47.5s = Fit runtime\n",
      "\t88 features in original data used to generate 88 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1503.77 MB (5.9% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 49.26s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/learner.pkl\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/utils/data/X.pkl\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/utils/data/y.pkl\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Model configs that will be trained (in order):\n",
      "\tKNeighborsUnif_BAG_L1: \t{'weights': 'uniform', 'ag_args': {'valid_stacker': False, 'problem_types': ['binary', 'multiclass', 'regression'], 'name_suffix': 'Unif', 'model_type': <class 'autogluon.tabular.models.knn.knn_model.KNNModel'>, 'priority': 100}, 'ag_args_fit': {'num_gpus': 1, 'num_stack_levels': 2}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tKNeighborsDist_BAG_L1: \t{'weights': 'distance', 'ag_args': {'valid_stacker': False, 'problem_types': ['binary', 'multiclass', 'regression'], 'name_suffix': 'Dist', 'model_type': <class 'autogluon.tabular.models.knn.knn_model.KNNModel'>, 'priority': 100}, 'ag_args_fit': {'num_gpus': 1, 'num_stack_levels': 2}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tLightGBMXT_BAG_L1: \t{'extra_trees': True, 'ag_args': {'name_suffix': 'XT', 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}, 'ag_args_fit': {'num_gpus': 1, 'num_stack_levels': 2}}\n",
      "\tLightGBM_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}, 'ag_args_fit': {'num_gpus': 1, 'num_stack_levels': 2}}\n",
      "\tRandomForestMSE_BAG_L1: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_fit': {'num_gpus': 1, 'num_stack_levels': 2}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tCatBoost_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}, 'ag_args_fit': {'num_gpus': 1, 'num_stack_levels': 2}}\n",
      "\tExtraTreesMSE_BAG_L1: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_fit': {'num_gpus': 1, 'num_stack_levels': 2}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetFastAI_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>, 'priority': 50}, 'ag_args_fit': {'num_gpus': 1, 'num_stack_levels': 2}}\n",
      "\tXGBoost_BAG_L1: \t{'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>, 'priority': 40}, 'ag_args_fit': {'num_gpus': 1, 'num_stack_levels': 2}}\n",
      "\tNeuralNetTorch_BAG_L1: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}, 'ag_args_fit': {'num_gpus': 1, 'num_stack_levels': 2}}\n",
      "\tLightGBMLarge_BAG_L1: \t{'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1, 'num_stack_levels': 2}}\n",
      "Fitting 11 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 28759.97s of the 43150.48s of remaining time.\n",
      "\tFitting KNeighborsUnif_BAG_L1 with 'num_gpus': 1, 'num_cpus': 32\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/KNeighborsUnif_BAG_L1/utils/model_template.pkl\n",
      "Loading: /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/KNeighborsUnif_BAG_L1/utils/model_template.pkl\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 5.741 GB out of 23.769 GB available memory (24.153%)... (20.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.26 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train KNeighborsUnif_BAG_L1... Skipping this model.\n",
      "Loading: /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/KNeighborsUnif_BAG_L1/utils/model_template.pkl\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/trainer.pkl\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 28718.00s of the 43108.62s of remaining time.\n",
      "\tFitting KNeighborsDist_BAG_L1 with 'num_gpus': 1, 'num_cpus': 32\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/KNeighborsDist_BAG_L1/utils/model_template.pkl\n",
      "Loading: /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/KNeighborsDist_BAG_L1/utils/model_template.pkl\n",
      "\tWarning: Not enough memory to safely train model. Estimated to require 5.741 GB out of 23.764 GB available memory (24.159%)... (20.000% of avail memory is the max safe size)\n",
      "\tTo force training the model, specify the model hyperparameter \"ag.max_memory_usage_ratio\" to a larger value (currently 1.0, set to >=1.26 to avoid the error)\n",
      "\t\tTo set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={\"ag.max_memory_usage_ratio\": VALUE})`\n",
      "\t\tSetting \"ag.max_memory_usage_ratio\" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.\n",
      "\tNot enough memory to train KNeighborsDist_BAG_L1... Skipping this model.\n",
      "Loading: /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/KNeighborsDist_BAG_L1/utils/model_template.pkl\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/trainer.pkl\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 28678.00s of the 43068.63s of remaining time.\n",
      "\tFitting LightGBMXT_BAG_L1 with 'num_gpus': 1, 'num_cpus': 32\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/LightGBMXT_BAG_L1/utils/model_template.pkl\n",
      "Loading: /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/LightGBMXT_BAG_L1/utils/model_template.pkl\n",
      "2025-01-12 19:44:37,354\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tMemory not enough to fit 5 folds in parallel. Will train 1 folds in parallel instead (Estimated 43.21% memory usage per fold, 43.21%/80.00% total).\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=43.21%)\n",
      "\t\tSwitching to pseudo sequential ParallelFoldFittingStrategy to avoid Python memory leakage.\n",
      "\t\tOverrule this behavior by setting fold_fitting_strategy to 'sequential_local' in ag_args_ensemble when when calling `predictor.fit`\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/LightGBMXT_BAG_L1/utils/oof.pkl\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/LightGBMXT_BAG_L1/model.pkl\n",
      "\t-0.8904\t = Validation score   (-root_mean_squared_error)\n",
      "\t57686.33s\t = Training   runtime\n",
      "\t1390.91s\t = Validation runtime\n",
      "\t644.1\t = Inference  throughput (rows/s | 895920 batch size)\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/trainer.pkl\n",
      "Skipping LightGBM_BAG_L1 due to lack of time remaining.\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/trainer.pkl\n",
      "Skipping RandomForestMSE_BAG_L1 due to lack of time remaining.\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/trainer.pkl\n",
      "Skipping CatBoost_BAG_L1 due to lack of time remaining.\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/trainer.pkl\n",
      "Skipping ExtraTreesMSE_BAG_L1 due to lack of time remaining.\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_BAG_L1 due to lack of time remaining.\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/trainer.pkl\n",
      "Skipping XGBoost_BAG_L1 due to lack of time remaining.\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_BAG_L1 due to lack of time remaining.\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/trainer.pkl\n",
      "Skipping LightGBMLarge_BAG_L1 due to lack of time remaining.\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/trainer.pkl\n",
      "Loading: /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/LightGBMXT_BAG_L1/utils/oof.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tWeightedEnsemble_L2: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 2876.00s of the -14972.70s of remaining time.\n",
      "\tFitting WeightedEnsemble_L2 with 'num_gpus': 0, 'num_cpus': 32\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/WeightedEnsemble_L2/utils/model_template.pkl\n",
      "Loading: /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/WeightedEnsemble_L2/utils/model_template.pkl\n",
      "Subsampling to 1000000 samples to speedup ensemble selection...\n",
      "Ensemble size: 1\n",
      "Ensemble weights: \n",
      "[1.]\n",
      "\t4.05s\t= Estimated out-of-fold prediction time...\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/WeightedEnsemble_L2/utils/oof.pkl\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/WeightedEnsemble_L2/model.pkl\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t-0.8904\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.14s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "\t644.1\t = Inference  throughput (rows/s | 895920 batch size)\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/trainer.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tLightGBMXT_BAG_L2: \t{'extra_trees': True, 'ag_args': {'name_suffix': 'XT', 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}, 'ag_args_fit': {'num_gpus': 1, 'num_stack_levels': 2}}\n",
      "\tLightGBM_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}, 'ag_args_fit': {'num_gpus': 1, 'num_stack_levels': 2}}\n",
      "\tRandomForestMSE_BAG_L2: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_fit': {'num_gpus': 1, 'num_stack_levels': 2}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tCatBoost_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}, 'ag_args_fit': {'num_gpus': 1, 'num_stack_levels': 2}}\n",
      "\tExtraTreesMSE_BAG_L2: \t{'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_fit': {'num_gpus': 1, 'num_stack_levels': 2}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
      "\tNeuralNetFastAI_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>, 'priority': 50}, 'ag_args_fit': {'num_gpus': 1, 'num_stack_levels': 2}}\n",
      "\tXGBoost_BAG_L2: \t{'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>, 'priority': 40}, 'ag_args_fit': {'num_gpus': 1, 'num_stack_levels': 2}}\n",
      "\tNeuralNetTorch_BAG_L2: \t{'ag_args': {'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}, 'ag_args_fit': {'num_gpus': 1, 'num_stack_levels': 2}}\n",
      "\tLightGBMLarge_BAG_L2: \t{'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}, 'ag_args_fit': {'num_gpus': 1, 'num_stack_levels': 2}}\n",
      "Fitting 9 L2 models, fit_strategy=\"sequential\" ...\n",
      "Loading: /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/LightGBMXT_BAG_L1/utils/oof.pkl\n",
      "Skipping LightGBMXT_BAG_L2 due to lack of time remaining.\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/trainer.pkl\n",
      "Skipping LightGBM_BAG_L2 due to lack of time remaining.\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/trainer.pkl\n",
      "Skipping RandomForestMSE_BAG_L2 due to lack of time remaining.\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/trainer.pkl\n",
      "Skipping CatBoost_BAG_L2 due to lack of time remaining.\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/trainer.pkl\n",
      "Skipping ExtraTreesMSE_BAG_L2 due to lack of time remaining.\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/trainer.pkl\n",
      "Skipping NeuralNetFastAI_BAG_L2 due to lack of time remaining.\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/trainer.pkl\n",
      "Skipping XGBoost_BAG_L2 due to lack of time remaining.\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/trainer.pkl\n",
      "Skipping NeuralNetTorch_BAG_L2 due to lack of time remaining.\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/trainer.pkl\n",
      "Skipping LightGBMLarge_BAG_L2 due to lack of time remaining.\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/trainer.pkl\n",
      "Loading: /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/LightGBMXT_BAG_L1/utils/oof.pkl\n",
      "Model configs that will be trained (in order):\n",
      "\tWeightedEnsemble_L3: \t{'ag_args': {'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -14976.79s of remaining time.\n",
      "\tFitting WeightedEnsemble_L3 with 'num_gpus': 0, 'num_cpus': 32\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/WeightedEnsemble_L3/utils/model_template.pkl\n",
      "Loading: /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/WeightedEnsemble_L3/utils/model_template.pkl\n",
      "Subsampling to 1000000 samples to speedup ensemble selection...\n",
      "Ensemble size: 1\n",
      "Ensemble weights: \n",
      "[1.]\n",
      "\t3.82s\t= Estimated out-of-fold prediction time...\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/WeightedEnsemble_L3/utils/oof.pkl\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/WeightedEnsemble_L3/model.pkl\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t-0.8904\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.13s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "\t644.1\t = Inference  throughput (rows/s | 895920 batch size)\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/trainer.pkl\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/trainer.pkl\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/trainer.pkl\n",
      "AutoGluon training complete, total runtime = 58183.01s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 644.1 rows/s (895920 batch size)\n",
      "Loading: /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/trainer.pkl\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/models/trainer.pkl\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/learner.pkl\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/predictor.pkl\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/version.txt with contents \"1.2\"\n",
      "Saving /mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301/metadata.json\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(\n",
    "    label=target,\n",
    "    sample_weight=weight_col,\n",
    "    verbosity=3,\n",
    "    log_to_file=True,\n",
    "    path=f'AutogluonModels_{int(time.time())}/',\n",
    "    problem_type='regression',\n",
    "    \n",
    ").fit(\n",
    "    train_data=training_data[train_features + [target, weight_col]],\n",
    "    time_limit=12 * 3600,\n",
    "    ag_args_fit={\n",
    "        'num_gpus': 1,\n",
    "        'num_stack_levels': 2\n",
    "    },\n",
    "    presets='medium_quality',\n",
    "    feature_metadata=feature_metadata,\n",
    "    auto_stack=True,\n",
    "    num_bag_folds=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = TabularPredictor.load(\"/mnt/h/Studia/magisterskie/1 sem/ProjektSemestralny/AutogluonModels_1736707301\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_test</th>\n",
       "      <th>weighted_r2</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_test</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_test_marginal</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>...</th>\n",
       "      <th>hyperparameters</th>\n",
       "      <th>hyperparameters_fit</th>\n",
       "      <th>ag_args_fit</th>\n",
       "      <th>features</th>\n",
       "      <th>compile_time</th>\n",
       "      <th>child_hyperparameters</th>\n",
       "      <th>child_hyperparameters_fit</th>\n",
       "      <th>child_ag_args_fit</th>\n",
       "      <th>ancestors</th>\n",
       "      <th>descendants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBMXT_BAG_L1</td>\n",
       "      <td>-0.778313</td>\n",
       "      <td>0.018066</td>\n",
       "      <td>-0.890442</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>61.672315</td>\n",
       "      <td>1390.907716</td>\n",
       "      <td>57686.332518</td>\n",
       "      <td>61.672315</td>\n",
       "      <td>1390.907716</td>\n",
       "      <td>...</td>\n",
       "      <td>{'use_orig_features': True, 'valid_stacker': T...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limi...</td>\n",
       "      <td>[feature_48, feature_03, feature_04, feature_1...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'learning_rate': 0.05, 'extra_trees': True}</td>\n",
       "      <td>{'num_boost_round': 9105}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[WeightedEnsemble_L2, WeightedEnsemble_L3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L3</td>\n",
       "      <td>-0.778313</td>\n",
       "      <td>0.018066</td>\n",
       "      <td>-0.890442</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>61.680500</td>\n",
       "      <td>1390.941053</td>\n",
       "      <td>57686.467386</td>\n",
       "      <td>0.008186</td>\n",
       "      <td>0.033336</td>\n",
       "      <td>...</td>\n",
       "      <td>{'use_orig_features': False, 'valid_stacker': ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limi...</td>\n",
       "      <td>[LightGBMXT_BAG_L1]</td>\n",
       "      <td>None</td>\n",
       "      <td>{'ensemble_size': 25, 'subsample_size': 1000000}</td>\n",
       "      <td>{'ensemble_size': 1}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limi...</td>\n",
       "      <td>[LightGBMXT_BAG_L1]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>-0.778313</td>\n",
       "      <td>0.018066</td>\n",
       "      <td>-0.890442</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>61.685861</td>\n",
       "      <td>1390.960119</td>\n",
       "      <td>57686.473538</td>\n",
       "      <td>0.013547</td>\n",
       "      <td>0.052403</td>\n",
       "      <td>...</td>\n",
       "      <td>{'use_orig_features': False, 'valid_stacker': ...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limi...</td>\n",
       "      <td>[LightGBMXT_BAG_L1]</td>\n",
       "      <td>None</td>\n",
       "      <td>{'ensemble_size': 25, 'subsample_size': 1000000}</td>\n",
       "      <td>{'ensemble_size': 1}</td>\n",
       "      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limi...</td>\n",
       "      <td>[LightGBMXT_BAG_L1]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  score_test  weighted_r2  score_val  \\\n",
       "0    LightGBMXT_BAG_L1   -0.778313     0.018066  -0.890442   \n",
       "1  WeightedEnsemble_L3   -0.778313     0.018066  -0.890442   \n",
       "2  WeightedEnsemble_L2   -0.778313     0.018066  -0.890442   \n",
       "\n",
       "               eval_metric  pred_time_test  pred_time_val      fit_time  \\\n",
       "0  root_mean_squared_error       61.672315    1390.907716  57686.332518   \n",
       "1  root_mean_squared_error       61.680500    1390.941053  57686.467386   \n",
       "2  root_mean_squared_error       61.685861    1390.960119  57686.473538   \n",
       "\n",
       "   pred_time_test_marginal  pred_time_val_marginal  ...  \\\n",
       "0                61.672315             1390.907716  ...   \n",
       "1                 0.008186                0.033336  ...   \n",
       "2                 0.013547                0.052403  ...   \n",
       "\n",
       "                                     hyperparameters  hyperparameters_fit  \\\n",
       "0  {'use_orig_features': True, 'valid_stacker': T...                   {}   \n",
       "1  {'use_orig_features': False, 'valid_stacker': ...                   {}   \n",
       "2  {'use_orig_features': False, 'valid_stacker': ...                   {}   \n",
       "\n",
       "                                         ag_args_fit  \\\n",
       "0  {'max_memory_usage_ratio': 1.0, 'max_time_limi...   \n",
       "1  {'max_memory_usage_ratio': 1.0, 'max_time_limi...   \n",
       "2  {'max_memory_usage_ratio': 1.0, 'max_time_limi...   \n",
       "\n",
       "                                            features  compile_time  \\\n",
       "0  [feature_48, feature_03, feature_04, feature_1...          None   \n",
       "1                                [LightGBMXT_BAG_L1]          None   \n",
       "2                                [LightGBMXT_BAG_L1]          None   \n",
       "\n",
       "                              child_hyperparameters  \\\n",
       "0      {'learning_rate': 0.05, 'extra_trees': True}   \n",
       "1  {'ensemble_size': 25, 'subsample_size': 1000000}   \n",
       "2  {'ensemble_size': 25, 'subsample_size': 1000000}   \n",
       "\n",
       "   child_hyperparameters_fit  \\\n",
       "0  {'num_boost_round': 9105}   \n",
       "1       {'ensemble_size': 1}   \n",
       "2       {'ensemble_size': 1}   \n",
       "\n",
       "                                   child_ag_args_fit            ancestors  \\\n",
       "0  {'max_memory_usage_ratio': 1.0, 'max_time_limi...                   []   \n",
       "1  {'max_memory_usage_ratio': 1.0, 'max_time_limi...  [LightGBMXT_BAG_L1]   \n",
       "2  {'max_memory_usage_ratio': 1.0, 'max_time_limi...  [LightGBMXT_BAG_L1]   \n",
       "\n",
       "                                  descendants  \n",
       "0  [WeightedEnsemble_L2, WeightedEnsemble_L3]  \n",
       "1                                          []  \n",
       "2                                          []  \n",
       "\n",
       "[3 rows x 36 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaderboard = predictor.leaderboard(validation_data, extra_metrics=[weighted_r2_scorer], extra_info=True)\n",
    "leaderboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoGluonVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
