{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/h/Studia/magisterskie/1_sem/ProjektSemestralny/HPOVenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-31 20:50:18,680\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-01-31 20:50:19,516\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import dask\n",
    "import dask.array as da\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.search.optuna import OptunaSearch\n",
    "from ray.air import session\n",
    "from ray.air.integrations.wandb import WandbLoggerCallback\n",
    "import shap\n",
    "\n",
    "import wandb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/kuba/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "2025-01-31 20:50:34,713\tINFO worker.py:1841 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x7f5003fd07f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()\n",
    "if not ray.is_initialized():\n",
    "    ray.init(ignore_reinit_error=True)\n",
    "\n",
    "client = Client()\n",
    "dask.config.set(scheduler=\"threads\")\n",
    "dask.config.set({\"dataframe.convert-string\": True,\n",
    "                 \"dataframe.shuffle.method\": \"tasks\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/h/Studia/magisterskie/1_sem/ProjektSemestralny/HPOVenv/lib/python3.10/site-packages/dask/base.py:1105: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "data_path = \"csv_partitions/*.csv\"\n",
    "df = dd.read_csv(data_path, assume_missing=True)\n",
    "\n",
    "df = df.sample(frac=0.02, random_state=42)\n",
    "\n",
    "unique_dates = df[\"date_id\"].unique().compute()\n",
    "unique_dates = np.sort(unique_dates)\n",
    "\n",
    "def make_time_series_folds(unique_dates, n_folds=3, min_train_size=1):\n",
    "    \"\"\"\n",
    "    Example time-series fold maker. \n",
    "    Adjust logic to ensure your folds are large enough.\n",
    "    \"\"\"\n",
    "    n_dates = len(unique_dates)\n",
    "    fold_size = n_dates // (n_folds + 1)\n",
    "\n",
    "    folds = []\n",
    "    for i in range(1, n_folds + 1):\n",
    "        train_end = i * fold_size\n",
    "        val_start = train_end\n",
    "        val_end = (i + 1) * fold_size if i < n_folds else n_dates\n",
    "\n",
    "        train_dates = unique_dates[:train_end]\n",
    "        val_dates = unique_dates[val_start:val_end]\n",
    "\n",
    "        if len(train_dates) < min_train_size:\n",
    "            raise ValueError(f\"Fold {i} has insufficient train data!\")\n",
    "        \n",
    "        folds.append((train_dates, val_dates))\n",
    "    return folds\n",
    "\n",
    "folds = make_time_series_folds(unique_dates, n_folds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm_model(X_train, y_train, w_train, \n",
    "                     X_val,   y_val,   w_val, \n",
    "                     model_params):\n",
    "    lgb_params = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"learning_rate\": model_params[\"learning_rate\"],\n",
    "        \"num_leaves\": model_params[\"num_leaves\"],\n",
    "        \"feature_fraction\": model_params[\"feature_fraction\"],\n",
    "        \"extra_trees\": model_params[\"extra_trees\"],\n",
    "        \"device_type\": \"cpu\",\n",
    "    }\n",
    "\n",
    "    dtrain = lgb.Dataset(X_train, label=y_train, weight=w_train)\n",
    "    dval   = lgb.Dataset(X_val,   label=y_val,   weight=w_val)\n",
    "\n",
    "    callbacks = [\n",
    "        lgb.early_stopping(stopping_rounds=100),\n",
    "        lgb.log_evaluation(period=100),\n",
    "    ]\n",
    "\n",
    "    model = lgb.train(\n",
    "        params=lgb_params,\n",
    "        train_set=dtrain,\n",
    "        num_boost_round=3000,\n",
    "        valid_sets=[dval],\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_xgb_model(X_train, y_train, w_train,\n",
    "                    X_val,   y_val,   w_val, \n",
    "                    model_params):\n",
    "    xgb_params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"eta\": model_params[\"learning_rate\"],\n",
    "        \"max_depth\": model_params[\"max_depth\"],\n",
    "        \"subsample\": model_params[\"subsample\"],\n",
    "        \"colsample_bytree\": model_params[\"colsample_bytree\"],\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"device\": \"cuda\",  # or \"cpu\" if no GPU\n",
    "    }\n",
    "\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train, weight=w_train)\n",
    "    dval   = xgb.DMatrix(X_val,   label=y_val,   weight=w_val)\n",
    "\n",
    "    model = xgb.train(\n",
    "        params=xgb_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=3000,\n",
    "        evals=[(dval, \"valid\")],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=100,\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_ensemble(base_train_preds_list, base_val_preds_list, y_train, w_train, y_val, w_val, weights):\n",
    "    \"\"\"\n",
    "    Direct weighted average (no additional meta-model).\n",
    "    weights: array-like of shape (n_base_models,) >= 0\n",
    "    If some model is \"unused\", pass weight=0 for it.\n",
    "    \"\"\"\n",
    "    weights = np.array(weights, dtype=float)\n",
    "\n",
    "    base_train_preds = np.column_stack(base_train_preds_list)\n",
    "    base_val_preds   = np.column_stack(base_val_preds_list)\n",
    "\n",
    "    wsum = np.sum(weights)\n",
    "    if wsum < 1e-12:  # guard against division by zero\n",
    "        wsum = float(base_val_preds.shape[1])\n",
    "        weights = np.ones_like(weights)\n",
    "\n",
    "    train_pred = np.average(base_train_preds, axis=1, weights=weights)\n",
    "    val_pred   = np.average(base_val_preds,   axis=1, weights=weights)\n",
    "\n",
    "    val_r2 = r2_score(y_val, val_pred, sample_weight=w_val)\n",
    "    return val_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_time_series_cv(config):\n",
    "    \"\"\"\n",
    "    - Time Series CV over 'folds'\n",
    "    - Weighted ensemble of up to 5 LGB + 5 XGB\n",
    "    - Optionally skip training some models by setting n_lgb_xgb\n",
    "    - Tuning imputer strategy\n",
    "    - Final metric = average R2 across folds\n",
    "    \"\"\"\n",
    "    n_lgb, n_xgb = config[\"n_lgb_xgb\"]\n",
    "    \n",
    "    feature_cols = [\n",
    "        f\"feature_{i:02d}\" for i in range(79)\n",
    "    ] + [f\"responder_{i}_lag_1\" for i in range(9)]\n",
    "    target_col = \"responder_6\"\n",
    "    weight_col = \"weight\"\n",
    "\n",
    "    imputer_strategy = config[\"imputer_strategy\"]\n",
    "    if imputer_strategy == \"constant\":\n",
    "        fill_val = config[\"imputer_fill_value\"]\n",
    "        imputer = SimpleImputer(strategy=\"constant\", fill_value=fill_val)\n",
    "    else:\n",
    "        imputer = SimpleImputer(strategy=imputer_strategy)\n",
    "\n",
    "    fold_r2_scores = []\n",
    "\n",
    "    for fold_i, (train_dates, val_dates) in enumerate(folds):\n",
    "        df_train_dd = df[df[\"date_id\"].isin(train_dates)]\n",
    "        df_val_dd   = df[df[\"date_id\"].isin(val_dates)]\n",
    "\n",
    "        df_train = df_train_dd[[*feature_cols, target_col, weight_col]].compute()\n",
    "        df_val   = df_val_dd[[*feature_cols, target_col, weight_col]].compute()\n",
    "\n",
    "        X_train = df_train[feature_cols].values\n",
    "        y_train = df_train[target_col].values\n",
    "        w_train = df_train[weight_col].values\n",
    "\n",
    "        X_val   = df_val[feature_cols].values\n",
    "        y_val   = df_val[target_col].values\n",
    "        w_val   = df_val[weight_col].values\n",
    "\n",
    "        X_train = imputer.fit_transform(X_train)\n",
    "        X_val   = imputer.transform(X_val)\n",
    "\n",
    "        base_train_preds_list = []\n",
    "        base_val_preds_list   = []\n",
    "\n",
    "        for i in range(1, n_lgb+1):\n",
    "            lgb_params = {\n",
    "                \"learning_rate\":    config[f\"lgb_{i}__learning_rate\"],\n",
    "                \"num_leaves\":       config[f\"lgb_{i}__num_leaves\"],\n",
    "                \"feature_fraction\": config[f\"lgb_{i}__feature_fraction\"],\n",
    "                \"extra_trees\":      config[f\"lgb_{i}__extra_trees\"],\n",
    "            }\n",
    "            model = train_lgbm_model(X_train, y_train, w_train,\n",
    "                                     X_val,   y_val,   w_val,\n",
    "                                     lgb_params)\n",
    "            base_train_preds_list.append(model.predict(X_train))\n",
    "            base_val_preds_list.append(model.predict(X_val))\n",
    "\n",
    "        for i in range(1, n_xgb+1):\n",
    "            xgb_params = {\n",
    "                \"learning_rate\":     config[f\"xgb_{i}__learning_rate\"],\n",
    "                \"max_depth\":         config[f\"xgb_{i}__max_depth\"],\n",
    "                \"subsample\":         config[f\"xgb_{i}__subsample\"],\n",
    "                \"colsample_bytree\":  config[f\"xgb_{i}__colsample_bytree\"],\n",
    "            }\n",
    "            xgb_model = train_xgb_model(X_train, y_train, w_train,\n",
    "                                        X_val,   y_val,   w_val,\n",
    "                                        xgb_params)\n",
    "            base_train_preds_list.append(xgb_model.predict(xgb.DMatrix(X_train)))\n",
    "            base_val_preds_list.append(xgb_model.predict(xgb.DMatrix(X_val)))\n",
    "\n",
    "        total_used = n_lgb + n_xgb\n",
    "        max_base_models = 10\n",
    "        while len(base_train_preds_list) < max_base_models:\n",
    "            base_train_preds_list.append(np.zeros_like(y_train, dtype=float))\n",
    "            base_val_preds_list.append(np.zeros_like(y_val,   dtype=float))\n",
    "\n",
    "        weights = []\n",
    "        for i in range(1, 6):\n",
    "            weights.append(config[f\"alpha_lgb_{i}\"])\n",
    "        for i in range(1, 6):\n",
    "            weights.append(config[f\"alpha_xgb_{i}\"])\n",
    "\n",
    "        fold_r2 = weighted_ensemble(\n",
    "            base_train_preds_list, base_val_preds_list,\n",
    "            y_train, w_train, y_val, w_val,\n",
    "            weights\n",
    "        )\n",
    "        fold_r2_scores.append(fold_r2)\n",
    "\n",
    "        del df_train, df_val, X_train, X_val\n",
    "        del y_train, y_val, w_train, w_val\n",
    "        del base_train_preds_list, base_val_preds_list\n",
    "\n",
    "    mean_r2 = np.mean(fold_r2_scores)\n",
    "    session.report({\"r2\": mean_r2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weighted_ensemble_search_space():\n",
    "    search_space = {}\n",
    "    \n",
    "    search_space[\"n_lgb_xgb\"] = tune.choice([(i, 5 - i) for i in range(6)])\n",
    "    \n",
    "    for i in range(1, 6):\n",
    "        prefix = f\"lgb_{i}\"\n",
    "        search_space[f\"{prefix}__learning_rate\"]    = tune.loguniform(0.001, 0.1)\n",
    "        search_space[f\"{prefix}__num_leaves\"]       = tune.randint(31, 128)\n",
    "        search_space[f\"{prefix}__feature_fraction\"] = tune.uniform(0.8, 1.0)\n",
    "        search_space[f\"{prefix}__extra_trees\"]      = tune.choice([False, True])\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        prefix = f\"xgb_{i}\"\n",
    "        search_space[f\"{prefix}__learning_rate\"]     = tune.loguniform(0.001, 0.1)\n",
    "        search_space[f\"{prefix}__max_depth\"]         = tune.randint(3, 15)\n",
    "        search_space[f\"{prefix}__subsample\"]         = tune.uniform(0.8, 1.0)\n",
    "        search_space[f\"{prefix}__colsample_bytree\"]  = tune.choice([0.8, 1.0])\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        search_space[f\"alpha_lgb_{i}\"] = tune.uniform(0.0, 3.0)\n",
    "    for i in range(1, 6):\n",
    "        search_space[f\"alpha_xgb_{i}\"] = tune.uniform(0.0, 3.0)\n",
    "\n",
    "    search_space[\"imputer_strategy\"] = tune.choice([\"mean\", \"median\", \"constant\"])\n",
    "    search_space[\"imputer_fill_value\"] = tune.choice([0, 3, 9999])\n",
    "\n",
    "    return search_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best config: {'n_lgb_xgb': [3, 2], 'lgb_1__learning_rate': 0.023155522539548737, 'lgb_1__num_leaves': 90, 'lgb_1__feature_fraction': 0.8253141645722654, 'lgb_1__extra_trees': False, 'lgb_2__learning_rate': 0.05942631254908533, 'lgb_2__num_leaves': 67, 'lgb_2__feature_fraction': 0.976443782221655, 'lgb_2__extra_trees': False, 'lgb_3__learning_rate': 0.011781883031179077, 'lgb_3__num_leaves': 36, 'lgb_3__feature_fraction': 0.8637813863872678, 'lgb_3__extra_trees': True, 'lgb_4__learning_rate': 0.006550833574950993, 'lgb_4__num_leaves': 31, 'lgb_4__feature_fraction': 0.8843708965935843, 'lgb_4__extra_trees': True, 'lgb_5__learning_rate': 0.032224080277769934, 'lgb_5__num_leaves': 126, 'lgb_5__feature_fraction': 0.8154497206585487, 'lgb_5__extra_trees': True, 'xgb_1__learning_rate': 0.020350451121469607, 'xgb_1__max_depth': 6, 'xgb_1__subsample': 0.8230163049324326, 'xgb_1__colsample_bytree': 1.0, 'xgb_2__learning_rate': 0.0011881006804406657, 'xgb_2__max_depth': 14, 'xgb_2__subsample': 0.8637880476969049, 'xgb_2__colsample_bytree': 1.0, 'xgb_3__learning_rate': 0.006663967089857414, 'xgb_3__max_depth': 5, 'xgb_3__subsample': 0.985774429788641, 'xgb_3__colsample_bytree': 0.8, 'xgb_4__learning_rate': 0.08368360509316912, 'xgb_4__max_depth': 10, 'xgb_4__subsample': 0.9558076624120088, 'xgb_4__colsample_bytree': 0.8, 'xgb_5__learning_rate': 0.04235003503689703, 'xgb_5__max_depth': 10, 'xgb_5__subsample': 0.866353568517836, 'xgb_5__colsample_bytree': 0.8, 'alpha_lgb_1': 2.258637637708287, 'alpha_lgb_2': 2.676613187378147, 'alpha_lgb_3': 2.7736675112767575, 'alpha_lgb_4': 0.7632264813850037, 'alpha_lgb_5': 0.3233043338649826, 'alpha_xgb_1': 1.5521866256424315, 'alpha_xgb_2': 0.3479610600249655, 'alpha_xgb_3': 0.8354502234535403, 'alpha_xgb_4': 0.5303279962715989, 'alpha_xgb_5': 2.0344497023958446, 'imputer_strategy': 'constant', 'imputer_fill_value': 9999}\n",
      "Best R2 Score: 0.26318463178245466\n"
     ]
    }
   ],
   "source": [
    "RUN_HPO = False\n",
    "base_dir = \"/mnt/h/Studia/magisterskie/1_sem/ProjektSemestralny\"\n",
    "result_dir = os.path.join(base_dir, \"ray_optuna_results\")\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "PROJECT_NAME = \"jane-street-HPO-weighted-ensv3\"\n",
    "\n",
    "if RUN_HPO:\n",
    "    search_space = create_weighted_ensemble_search_space()\n",
    "    \n",
    "    optuna_search = OptunaSearch(metric=\"r2\", mode=\"max\")\n",
    "\n",
    "    wandb_callback = WandbLoggerCallback(\n",
    "        project=PROJECT_NAME,\n",
    "        entity=\"kubston20004\",\n",
    "        log_config=True\n",
    "    )\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        tune.with_resources(\n",
    "            train_with_time_series_cv,\n",
    "            resources={\"cpu\": int(os.cpu_count() * 0.8), \"gpu\": 1}\n",
    "        ),\n",
    "        param_space=search_space,\n",
    "        tune_config=tune.TuneConfig(\n",
    "            search_alg=optuna_search,\n",
    "            num_samples=6,\n",
    "            max_concurrent_trials=2,\n",
    "        ),\n",
    "        run_config=ray.air.RunConfig(\n",
    "            name=PROJECT_NAME,\n",
    "            storage_path=result_dir,\n",
    "            callbacks=[wandb_callback],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    results = tuner.fit()\n",
    "else:\n",
    "    restored_tuner = tune.Tuner.restore(\n",
    "        trainable=train_with_time_series_cv\n",
    "    )\n",
    "    results = restored_tuner.get_results()\n",
    "\n",
    "best_result = results.get_best_result(metric=\"r2\", mode=\"max\")\n",
    "print(\"Best config:\", best_result.config)\n",
    "print(\"Best R2 Score:\", best_result.metrics[\"r2\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HPOVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
